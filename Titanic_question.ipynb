{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T14:17:59.663150Z",
     "start_time": "2026-01-31T14:17:59.650994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import reverse_geocoder as rg"
   ],
   "id": "926377e171581bde",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "2aca6851cd016351"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T14:23:50.061790Z",
     "start_time": "2026-01-31T14:23:49.974215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Data =pd.read_csv(\"housing.csv\")\n",
    "X = Data.drop(['median_house_value'] , axis=1)\n",
    "Y = Data['median_house_value']\n",
    "Y = Y.values.reshape(-1,1)\n",
    "\n",
    "X['city'] = rg.search(list(zip(X['latitude'],X['longitude'])), mode=1)[0]['name']\n",
    "\n",
    "X =X.drop(['latitude','longitude','city'], axis=1)\n",
    "X\n"
   ],
   "id": "17e902e38ea5f5e3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       housing_median_age  total_rooms  total_bedrooms  population  \\\n",
       "0                    41.0        880.0           129.0       322.0   \n",
       "1                    21.0       7099.0          1106.0      2401.0   \n",
       "2                    52.0       1467.0           190.0       496.0   \n",
       "3                    52.0       1274.0           235.0       558.0   \n",
       "4                    52.0       1627.0           280.0       565.0   \n",
       "...                   ...          ...             ...         ...   \n",
       "20635                25.0       1665.0           374.0       845.0   \n",
       "20636                18.0        697.0           150.0       356.0   \n",
       "20637                17.0       2254.0           485.0      1007.0   \n",
       "20638                18.0       1860.0           409.0       741.0   \n",
       "20639                16.0       2785.0           616.0      1387.0   \n",
       "\n",
       "       households  median_income ocean_proximity  \n",
       "0           126.0         8.3252        NEAR BAY  \n",
       "1          1138.0         8.3014        NEAR BAY  \n",
       "2           177.0         7.2574        NEAR BAY  \n",
       "3           219.0         5.6431        NEAR BAY  \n",
       "4           259.0         3.8462        NEAR BAY  \n",
       "...           ...            ...             ...  \n",
       "20635       330.0         1.5603          INLAND  \n",
       "20636       114.0         2.5568          INLAND  \n",
       "20637       433.0         1.7000          INLAND  \n",
       "20638       349.0         1.8672          INLAND  \n",
       "20639       530.0         2.3886          INLAND  \n",
       "\n",
       "[20640 rows x 7 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20635</th>\n",
       "      <td>25.0</td>\n",
       "      <td>1665.0</td>\n",
       "      <td>374.0</td>\n",
       "      <td>845.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>1.5603</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20636</th>\n",
       "      <td>18.0</td>\n",
       "      <td>697.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>2.5568</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20637</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2254.0</td>\n",
       "      <td>485.0</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>433.0</td>\n",
       "      <td>1.7000</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20638</th>\n",
       "      <td>18.0</td>\n",
       "      <td>1860.0</td>\n",
       "      <td>409.0</td>\n",
       "      <td>741.0</td>\n",
       "      <td>349.0</td>\n",
       "      <td>1.8672</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20639</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2785.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>530.0</td>\n",
       "      <td>2.3886</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20640 rows Ã— 7 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T14:23:50.853094Z",
     "start_time": "2026-01-31T14:23:50.806459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Place=['INLAND','ISLAND','NEAR BAY','<1H OCEAN','NEAR OCEAN']\n",
    "\n",
    "encoder = OrdinalEncoder(\n",
    "    categories=[Place],\n",
    "    handle_unknown='use_encoded_value',\n",
    "    unknown_value=-1\n",
    ")\n",
    "cat_cols = ['ocean_proximity']\n",
    "new_cols = ['Place']\n",
    "encoder.fit(X[cat_cols])\n",
    "\n",
    "X[new_cols] = encoder.transform(X[cat_cols])\n",
    "\n",
    "X = X.select_dtypes(include=['number'])\n",
    "\n",
    "\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.15,random_state=42)\n",
    "\n",
    "median_value_train = X_train.median(numeric_only = True)\n",
    "X_train = X_train.fillna(median_value_train)\n",
    "\n",
    "median_value_test =  X_test.median(numeric_only = True)\n",
    "X_test = X_test.fillna(median_value_test)\n",
    "\n",
    "\n",
    "poly = PolynomialFeatures(degree = 2,include_bias = False)\n",
    "X_poly = poly.fit_transform(X_train)\n",
    "poly_features = pd.DataFrame(X_poly, columns = poly.get_feature_names_out(X_train.columns))\n",
    "X_train = poly_features\n",
    "poly_test = PolynomialFeatures(degree = 2,include_bias = False)\n",
    "X_poly_2 = poly_test.fit_transform(X_test)\n",
    "X_test = X_poly_2\n",
    "\n",
    "X_test"
   ],
   "id": "3395ece63bea9194",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.50000000e+01, 1.50500000e+03, 4.27000000e+02, ...,\n",
       "        2.82643344e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [3.00000000e+01, 2.94300000e+03, 4.27000000e+02, ...,\n",
       "        6.40747969e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [5.20000000e+01, 3.83000000e+03, 4.27000000e+02, ...,\n",
       "        1.21110960e+01, 6.96020000e+00, 4.00000000e+00],\n",
       "       ...,\n",
       "       [4.60000000e+01, 2.06200000e+03, 4.84000000e+02, ...,\n",
       "        9.52956900e+00, 1.23480000e+01, 1.60000000e+01],\n",
       "       [1.70000000e+01, 8.89000000e+02, 1.31000000e+02, ...,\n",
       "        3.77315348e+01, 0.00000000e+00, 0.00000000e+00],\n",
       "       [3.70000000e+01, 1.80100000e+03, 4.22000000e+02, ...,\n",
       "        9.98370409e+00, 9.47910000e+00, 9.00000000e+00]], shape=(3096, 35))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T14:24:13.834858Z",
     "start_time": "2026-01-31T14:24:13.806745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "y_scaler = StandardScaler()\n",
    "y_scaler.fit(Y_train)\n",
    "Y_train = y_scaler.transform(Y_train)"
   ],
   "id": "91dc86c534fee383",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.3152842 , -0.96722769, -0.50206321, ..., -0.78705162,\n",
       "        -0.62495091,  0.47674295],\n",
       "       [ 0.27101546, -0.07830509,  0.03333907, ..., -0.18136745,\n",
       "         0.36439793,  0.47674295],\n",
       "       [ 1.06416529, -0.99941836, -1.06375758, ..., -0.39181599,\n",
       "        -1.07745284, -1.213329  ],\n",
       "       ...,\n",
       "       [ 0.58827539, -0.24707622,  0.07397228, ..., -0.45832614,\n",
       "         0.0279598 ,  0.47674295],\n",
       "       [-1.07733925,  0.43076749,  0.14089757, ...,  0.64686396,\n",
       "         1.07701689,  0.47674295],\n",
       "       [ 1.85731512,  0.7301408 ,  1.85705311, ..., -0.54901696,\n",
       "        -0.4306448 , -0.46218591]], shape=(17544, 35))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T14:24:54.436420Z",
     "start_time": "2026-01-31T14:24:54.424330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train_tensor = torch.tensor(X_train,dtype = torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test,dtype = torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train,dtype = torch.float32).view(-1,1)\n",
    "Y_test_tensor = torch.tensor(Y_test,dtype = torch.float32).view(-1,1)\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset,batch_size=64,shuffle=True)\n",
    "test_loader = DataLoader(test_dataset,batch_size=64)"
   ],
   "id": "da560f20604eab86",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T14:24:54.898631Z",
     "start_time": "2026-01-31T14:24:54.894387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "NUM_EPOCHS =500\n",
    "RANDOM_SEED = 42\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ],
   "id": "3892fd6e8a96202c",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T14:29:43.126700Z",
     "start_time": "2026-01-31T14:26:11.705791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self,num_features):\n",
    "        super().__init__()\n",
    "        self.my_net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=num_features, out_features=30),\n",
    "            torch.nn.BatchNorm1d(num_features=30),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=30, out_features=20),\n",
    "            torch.nn.BatchNorm1d(num_features=20),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=20, out_features=10),\n",
    "            torch.nn.BatchNorm1d(num_features=10),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=10, out_features=1),\n",
    "\n",
    "\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.my_net(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "model = Model(num_features=35)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "\n",
    "def compute_loss(net,dataloader):\n",
    "    curr_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for cnt , (fet, output) in enumerate(dataloader):\n",
    "            fet = fet.view((-1, 35)).to(DEVICE)\n",
    "            output = output.view((-1,1)).to(DEVICE)\n",
    "            out = net(fet)\n",
    "            loss = F.mse_loss(out,output)\n",
    "            curr_loss += loss\n",
    "        return float(curr_loss)/len(dataloader)\n",
    "\n",
    "start_time = time.time()\n",
    "minibatch_cost = []\n",
    "epoch_cost = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    for batch_idx , (features , targets) in enumerate(train_loader):\n",
    "        features = features.view(-1,35).to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "        pred = model.forward(features)\n",
    "        cost = F.mse_loss(pred,targets.float().view(-1,1))\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        minibatch_cost.append(cost.item())\n",
    "        if not batch_idx % 64:\n",
    "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f'\n",
    "                   %(epoch+1, NUM_EPOCHS, batch_idx,\n",
    "                     len(train_loader), cost.item()))\n",
    "    cost = compute_loss(model, train_loader)\n",
    "    epoch_cost.append(cost)\n",
    "    print('Epoch: %03d/%03d Train Cost: %.4f' % (\n",
    "            epoch+1, NUM_EPOCHS, cost))\n",
    "    print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "a3a8491c894e0544",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/500 | Batch 000/275 | Cost: 1.1495\n",
      "Epoch: 001/500 | Batch 064/275 | Cost: 0.2907\n",
      "Epoch: 001/500 | Batch 128/275 | Cost: 0.4975\n",
      "Epoch: 001/500 | Batch 192/275 | Cost: 0.2954\n",
      "Epoch: 001/500 | Batch 256/275 | Cost: 0.2202\n",
      "Epoch: 001/500 Train Cost: 0.3164\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 002/500 | Batch 000/275 | Cost: 0.2702\n",
      "Epoch: 002/500 | Batch 064/275 | Cost: 0.3011\n",
      "Epoch: 002/500 | Batch 128/275 | Cost: 0.2470\n",
      "Epoch: 002/500 | Batch 192/275 | Cost: 0.2660\n",
      "Epoch: 002/500 | Batch 256/275 | Cost: 0.3385\n",
      "Epoch: 002/500 Train Cost: 0.2962\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 003/500 | Batch 000/275 | Cost: 0.3153\n",
      "Epoch: 003/500 | Batch 064/275 | Cost: 0.2786\n",
      "Epoch: 003/500 | Batch 128/275 | Cost: 0.3510\n",
      "Epoch: 003/500 | Batch 192/275 | Cost: 0.2429\n",
      "Epoch: 003/500 | Batch 256/275 | Cost: 0.2589\n",
      "Epoch: 003/500 Train Cost: 0.2934\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 004/500 | Batch 000/275 | Cost: 0.3727\n",
      "Epoch: 004/500 | Batch 064/275 | Cost: 0.4883\n",
      "Epoch: 004/500 | Batch 128/275 | Cost: 0.2371\n",
      "Epoch: 004/500 | Batch 192/275 | Cost: 0.2863\n",
      "Epoch: 004/500 | Batch 256/275 | Cost: 0.2106\n",
      "Epoch: 004/500 Train Cost: 0.2861\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 005/500 | Batch 000/275 | Cost: 0.3794\n",
      "Epoch: 005/500 | Batch 064/275 | Cost: 0.2955\n",
      "Epoch: 005/500 | Batch 128/275 | Cost: 0.2285\n",
      "Epoch: 005/500 | Batch 192/275 | Cost: 0.4634\n",
      "Epoch: 005/500 | Batch 256/275 | Cost: 0.2185\n",
      "Epoch: 005/500 Train Cost: 0.2805\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 006/500 | Batch 000/275 | Cost: 0.2629\n",
      "Epoch: 006/500 | Batch 064/275 | Cost: 0.3398\n",
      "Epoch: 006/500 | Batch 128/275 | Cost: 0.3005\n",
      "Epoch: 006/500 | Batch 192/275 | Cost: 0.2932\n",
      "Epoch: 006/500 | Batch 256/275 | Cost: 0.3837\n",
      "Epoch: 006/500 Train Cost: 0.2842\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 007/500 | Batch 000/275 | Cost: 0.2572\n",
      "Epoch: 007/500 | Batch 064/275 | Cost: 0.2272\n",
      "Epoch: 007/500 | Batch 128/275 | Cost: 0.2301\n",
      "Epoch: 007/500 | Batch 192/275 | Cost: 0.3713\n",
      "Epoch: 007/500 | Batch 256/275 | Cost: 0.2976\n",
      "Epoch: 007/500 Train Cost: 0.2756\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 008/500 | Batch 000/275 | Cost: 0.2842\n",
      "Epoch: 008/500 | Batch 064/275 | Cost: 0.2389\n",
      "Epoch: 008/500 | Batch 128/275 | Cost: 0.5092\n",
      "Epoch: 008/500 | Batch 192/275 | Cost: 0.2488\n",
      "Epoch: 008/500 | Batch 256/275 | Cost: 0.3565\n",
      "Epoch: 008/500 Train Cost: 0.2733\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 009/500 | Batch 000/275 | Cost: 0.1477\n",
      "Epoch: 009/500 | Batch 064/275 | Cost: 0.1871\n",
      "Epoch: 009/500 | Batch 128/275 | Cost: 0.3672\n",
      "Epoch: 009/500 | Batch 192/275 | Cost: 0.1703\n",
      "Epoch: 009/500 | Batch 256/275 | Cost: 0.2940\n",
      "Epoch: 009/500 Train Cost: 0.2729\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 010/500 | Batch 000/275 | Cost: 0.2710\n",
      "Epoch: 010/500 | Batch 064/275 | Cost: 0.2206\n",
      "Epoch: 010/500 | Batch 128/275 | Cost: 0.2364\n",
      "Epoch: 010/500 | Batch 192/275 | Cost: 0.3111\n",
      "Epoch: 010/500 | Batch 256/275 | Cost: 0.3499\n",
      "Epoch: 010/500 Train Cost: 0.2772\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 011/500 | Batch 000/275 | Cost: 0.2236\n",
      "Epoch: 011/500 | Batch 064/275 | Cost: 0.2868\n",
      "Epoch: 011/500 | Batch 128/275 | Cost: 0.3590\n",
      "Epoch: 011/500 | Batch 192/275 | Cost: 0.2129\n",
      "Epoch: 011/500 | Batch 256/275 | Cost: 0.2916\n",
      "Epoch: 011/500 Train Cost: 0.2692\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 012/500 | Batch 000/275 | Cost: 0.3631\n",
      "Epoch: 012/500 | Batch 064/275 | Cost: 0.3930\n",
      "Epoch: 012/500 | Batch 128/275 | Cost: 0.2240\n",
      "Epoch: 012/500 | Batch 192/275 | Cost: 0.3590\n",
      "Epoch: 012/500 | Batch 256/275 | Cost: 0.2733\n",
      "Epoch: 012/500 Train Cost: 0.2708\n",
      "Time elapsed: 0.24 min\n",
      "Epoch: 013/500 | Batch 000/275 | Cost: 0.3103\n",
      "Epoch: 013/500 | Batch 064/275 | Cost: 0.1346\n",
      "Epoch: 013/500 | Batch 128/275 | Cost: 0.5039\n",
      "Epoch: 013/500 | Batch 192/275 | Cost: 0.3976\n",
      "Epoch: 013/500 | Batch 256/275 | Cost: 0.2338\n",
      "Epoch: 013/500 Train Cost: 0.2742\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 014/500 | Batch 000/275 | Cost: 0.1959\n",
      "Epoch: 014/500 | Batch 064/275 | Cost: 0.3862\n",
      "Epoch: 014/500 | Batch 128/275 | Cost: 0.2535\n",
      "Epoch: 014/500 | Batch 192/275 | Cost: 0.5868\n",
      "Epoch: 014/500 | Batch 256/275 | Cost: 0.2488\n",
      "Epoch: 014/500 Train Cost: 0.2662\n",
      "Time elapsed: 0.28 min\n",
      "Epoch: 015/500 | Batch 000/275 | Cost: 0.3881\n",
      "Epoch: 015/500 | Batch 064/275 | Cost: 0.3846\n",
      "Epoch: 015/500 | Batch 128/275 | Cost: 0.1965\n",
      "Epoch: 015/500 | Batch 192/275 | Cost: 0.4970\n",
      "Epoch: 015/500 | Batch 256/275 | Cost: 0.1842\n",
      "Epoch: 015/500 Train Cost: 0.2701\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 016/500 | Batch 000/275 | Cost: 0.3199\n",
      "Epoch: 016/500 | Batch 064/275 | Cost: 0.3078\n",
      "Epoch: 016/500 | Batch 128/275 | Cost: 0.1824\n",
      "Epoch: 016/500 | Batch 192/275 | Cost: 0.2874\n",
      "Epoch: 016/500 | Batch 256/275 | Cost: 0.2727\n",
      "Epoch: 016/500 Train Cost: 0.2650\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 017/500 | Batch 000/275 | Cost: 0.1653\n",
      "Epoch: 017/500 | Batch 064/275 | Cost: 0.2143\n",
      "Epoch: 017/500 | Batch 128/275 | Cost: 0.2587\n",
      "Epoch: 017/500 | Batch 192/275 | Cost: 0.1881\n",
      "Epoch: 017/500 | Batch 256/275 | Cost: 0.2357\n",
      "Epoch: 017/500 Train Cost: 0.2701\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 018/500 | Batch 000/275 | Cost: 0.1488\n",
      "Epoch: 018/500 | Batch 064/275 | Cost: 0.2394\n",
      "Epoch: 018/500 | Batch 128/275 | Cost: 0.4488\n",
      "Epoch: 018/500 | Batch 192/275 | Cost: 0.2084\n",
      "Epoch: 018/500 | Batch 256/275 | Cost: 0.2042\n",
      "Epoch: 018/500 Train Cost: 0.2700\n",
      "Time elapsed: 0.47 min\n",
      "Epoch: 019/500 | Batch 000/275 | Cost: 0.1820\n",
      "Epoch: 019/500 | Batch 064/275 | Cost: 0.1882\n",
      "Epoch: 019/500 | Batch 128/275 | Cost: 0.2931\n",
      "Epoch: 019/500 | Batch 192/275 | Cost: 0.3664\n",
      "Epoch: 019/500 | Batch 256/275 | Cost: 0.2279\n",
      "Epoch: 019/500 Train Cost: 0.2684\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 020/500 | Batch 000/275 | Cost: 0.3156\n",
      "Epoch: 020/500 | Batch 064/275 | Cost: 0.3997\n",
      "Epoch: 020/500 | Batch 128/275 | Cost: 0.2418\n",
      "Epoch: 020/500 | Batch 192/275 | Cost: 0.1756\n",
      "Epoch: 020/500 | Batch 256/275 | Cost: 0.2868\n",
      "Epoch: 020/500 Train Cost: 0.2717\n",
      "Time elapsed: 0.50 min\n",
      "Epoch: 021/500 | Batch 000/275 | Cost: 0.2954\n",
      "Epoch: 021/500 | Batch 064/275 | Cost: 0.4085\n",
      "Epoch: 021/500 | Batch 128/275 | Cost: 0.2955\n",
      "Epoch: 021/500 | Batch 192/275 | Cost: 0.1013\n",
      "Epoch: 021/500 | Batch 256/275 | Cost: 0.2186\n",
      "Epoch: 021/500 Train Cost: 0.2655\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 022/500 | Batch 000/275 | Cost: 0.2111\n",
      "Epoch: 022/500 | Batch 064/275 | Cost: 0.2331\n",
      "Epoch: 022/500 | Batch 128/275 | Cost: 0.2169\n",
      "Epoch: 022/500 | Batch 192/275 | Cost: 0.3124\n",
      "Epoch: 022/500 | Batch 256/275 | Cost: 0.2031\n",
      "Epoch: 022/500 Train Cost: 0.2675\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 023/500 | Batch 000/275 | Cost: 0.3112\n",
      "Epoch: 023/500 | Batch 064/275 | Cost: 0.3405\n",
      "Epoch: 023/500 | Batch 128/275 | Cost: 0.1705\n",
      "Epoch: 023/500 | Batch 192/275 | Cost: 0.2351\n",
      "Epoch: 023/500 | Batch 256/275 | Cost: 0.1580\n",
      "Epoch: 023/500 Train Cost: 0.2666\n",
      "Time elapsed: 0.56 min\n",
      "Epoch: 024/500 | Batch 000/275 | Cost: 0.2546\n",
      "Epoch: 024/500 | Batch 064/275 | Cost: 0.1854\n",
      "Epoch: 024/500 | Batch 128/275 | Cost: 0.3023\n",
      "Epoch: 024/500 | Batch 192/275 | Cost: 0.3193\n",
      "Epoch: 024/500 | Batch 256/275 | Cost: 0.1969\n",
      "Epoch: 024/500 Train Cost: 0.2594\n",
      "Time elapsed: 0.58 min\n",
      "Epoch: 025/500 | Batch 000/275 | Cost: 0.2566\n",
      "Epoch: 025/500 | Batch 064/275 | Cost: 0.2228\n",
      "Epoch: 025/500 | Batch 128/275 | Cost: 0.3160\n",
      "Epoch: 025/500 | Batch 192/275 | Cost: 0.2671\n",
      "Epoch: 025/500 | Batch 256/275 | Cost: 0.2780\n",
      "Epoch: 025/500 Train Cost: 0.2612\n",
      "Time elapsed: 0.59 min\n",
      "Epoch: 026/500 | Batch 000/275 | Cost: 0.1948\n",
      "Epoch: 026/500 | Batch 064/275 | Cost: 0.1806\n",
      "Epoch: 026/500 | Batch 128/275 | Cost: 0.2778\n",
      "Epoch: 026/500 | Batch 192/275 | Cost: 0.3774\n",
      "Epoch: 026/500 | Batch 256/275 | Cost: 0.2219\n",
      "Epoch: 026/500 Train Cost: 0.2608\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 027/500 | Batch 000/275 | Cost: 0.3061\n",
      "Epoch: 027/500 | Batch 064/275 | Cost: 0.4601\n",
      "Epoch: 027/500 | Batch 128/275 | Cost: 0.2473\n",
      "Epoch: 027/500 | Batch 192/275 | Cost: 0.3025\n",
      "Epoch: 027/500 | Batch 256/275 | Cost: 0.2553\n",
      "Epoch: 027/500 Train Cost: 0.2645\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 028/500 | Batch 000/275 | Cost: 0.3241\n",
      "Epoch: 028/500 | Batch 064/275 | Cost: 0.3117\n",
      "Epoch: 028/500 | Batch 128/275 | Cost: 0.1438\n",
      "Epoch: 028/500 | Batch 192/275 | Cost: 0.2278\n",
      "Epoch: 028/500 | Batch 256/275 | Cost: 0.2755\n",
      "Epoch: 028/500 Train Cost: 0.2614\n",
      "Time elapsed: 0.81 min\n",
      "Epoch: 029/500 | Batch 000/275 | Cost: 0.2650\n",
      "Epoch: 029/500 | Batch 064/275 | Cost: 0.3786\n",
      "Epoch: 029/500 | Batch 128/275 | Cost: 0.2830\n",
      "Epoch: 029/500 | Batch 192/275 | Cost: 0.1773\n",
      "Epoch: 029/500 | Batch 256/275 | Cost: 0.1922\n",
      "Epoch: 029/500 Train Cost: 0.2620\n",
      "Time elapsed: 0.88 min\n",
      "Epoch: 030/500 | Batch 000/275 | Cost: 0.5760\n",
      "Epoch: 030/500 | Batch 064/275 | Cost: 0.3611\n",
      "Epoch: 030/500 | Batch 128/275 | Cost: 0.2482\n",
      "Epoch: 030/500 | Batch 192/275 | Cost: 0.1642\n",
      "Epoch: 030/500 | Batch 256/275 | Cost: 0.2239\n",
      "Epoch: 030/500 Train Cost: 0.2612\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 031/500 | Batch 000/275 | Cost: 0.2961\n",
      "Epoch: 031/500 | Batch 064/275 | Cost: 0.2088\n",
      "Epoch: 031/500 | Batch 128/275 | Cost: 0.2365\n",
      "Epoch: 031/500 | Batch 192/275 | Cost: 0.1759\n",
      "Epoch: 031/500 | Batch 256/275 | Cost: 0.2453\n",
      "Epoch: 031/500 Train Cost: 0.2547\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 032/500 | Batch 000/275 | Cost: 0.2695\n",
      "Epoch: 032/500 | Batch 064/275 | Cost: 0.2922\n",
      "Epoch: 032/500 | Batch 128/275 | Cost: 0.2728\n",
      "Epoch: 032/500 | Batch 192/275 | Cost: 0.1804\n",
      "Epoch: 032/500 | Batch 256/275 | Cost: 0.3335\n",
      "Epoch: 032/500 Train Cost: 0.2603\n",
      "Time elapsed: 0.94 min\n",
      "Epoch: 033/500 | Batch 000/275 | Cost: 0.2536\n",
      "Epoch: 033/500 | Batch 064/275 | Cost: 0.2017\n",
      "Epoch: 033/500 | Batch 128/275 | Cost: 0.2051\n",
      "Epoch: 033/500 | Batch 192/275 | Cost: 0.2763\n",
      "Epoch: 033/500 | Batch 256/275 | Cost: 0.2182\n",
      "Epoch: 033/500 Train Cost: 0.2580\n",
      "Time elapsed: 0.96 min\n",
      "Epoch: 034/500 | Batch 000/275 | Cost: 0.3995\n",
      "Epoch: 034/500 | Batch 064/275 | Cost: 0.2391\n",
      "Epoch: 034/500 | Batch 128/275 | Cost: 0.1852\n",
      "Epoch: 034/500 | Batch 192/275 | Cost: 0.1837\n",
      "Epoch: 034/500 | Batch 256/275 | Cost: 0.1189\n",
      "Epoch: 034/500 Train Cost: 0.2604\n",
      "Time elapsed: 0.98 min\n",
      "Epoch: 035/500 | Batch 000/275 | Cost: 0.3133\n",
      "Epoch: 035/500 | Batch 064/275 | Cost: 0.6418\n",
      "Epoch: 035/500 | Batch 128/275 | Cost: 0.1410\n",
      "Epoch: 035/500 | Batch 192/275 | Cost: 0.2452\n",
      "Epoch: 035/500 | Batch 256/275 | Cost: 0.1527\n",
      "Epoch: 035/500 Train Cost: 0.2582\n",
      "Time elapsed: 1.00 min\n",
      "Epoch: 036/500 | Batch 000/275 | Cost: 0.1577\n",
      "Epoch: 036/500 | Batch 064/275 | Cost: 0.2356\n",
      "Epoch: 036/500 | Batch 128/275 | Cost: 0.3473\n",
      "Epoch: 036/500 | Batch 192/275 | Cost: 0.3386\n",
      "Epoch: 036/500 | Batch 256/275 | Cost: 0.3135\n",
      "Epoch: 036/500 Train Cost: 0.2617\n",
      "Time elapsed: 1.02 min\n",
      "Epoch: 037/500 | Batch 000/275 | Cost: 0.2747\n",
      "Epoch: 037/500 | Batch 064/275 | Cost: 0.3516\n",
      "Epoch: 037/500 | Batch 128/275 | Cost: 0.2536\n",
      "Epoch: 037/500 | Batch 192/275 | Cost: 0.2484\n",
      "Epoch: 037/500 | Batch 256/275 | Cost: 0.1822\n",
      "Epoch: 037/500 Train Cost: 0.2534\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 038/500 | Batch 000/275 | Cost: 0.2755\n",
      "Epoch: 038/500 | Batch 064/275 | Cost: 0.3050\n",
      "Epoch: 038/500 | Batch 128/275 | Cost: 0.2311\n",
      "Epoch: 038/500 | Batch 192/275 | Cost: 0.2503\n",
      "Epoch: 038/500 | Batch 256/275 | Cost: 0.2592\n",
      "Epoch: 038/500 Train Cost: 0.2583\n",
      "Time elapsed: 1.05 min\n",
      "Epoch: 039/500 | Batch 000/275 | Cost: 0.3070\n",
      "Epoch: 039/500 | Batch 064/275 | Cost: 0.4489\n",
      "Epoch: 039/500 | Batch 128/275 | Cost: 0.1559\n",
      "Epoch: 039/500 | Batch 192/275 | Cost: 0.2988\n",
      "Epoch: 039/500 | Batch 256/275 | Cost: 0.3174\n",
      "Epoch: 039/500 Train Cost: 0.2555\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 040/500 | Batch 000/275 | Cost: 0.1680\n",
      "Epoch: 040/500 | Batch 064/275 | Cost: 0.1001\n",
      "Epoch: 040/500 | Batch 128/275 | Cost: 0.2036\n",
      "Epoch: 040/500 | Batch 192/275 | Cost: 0.2041\n",
      "Epoch: 040/500 | Batch 256/275 | Cost: 0.3999\n",
      "Epoch: 040/500 Train Cost: 0.2615\n",
      "Time elapsed: 1.09 min\n",
      "Epoch: 041/500 | Batch 000/275 | Cost: 0.3589\n",
      "Epoch: 041/500 | Batch 064/275 | Cost: 0.3240\n",
      "Epoch: 041/500 | Batch 128/275 | Cost: 0.2325\n",
      "Epoch: 041/500 | Batch 192/275 | Cost: 0.2433\n",
      "Epoch: 041/500 | Batch 256/275 | Cost: 0.2164\n",
      "Epoch: 041/500 Train Cost: 0.2569\n",
      "Time elapsed: 1.11 min\n",
      "Epoch: 042/500 | Batch 000/275 | Cost: 0.1658\n",
      "Epoch: 042/500 | Batch 064/275 | Cost: 0.3627\n",
      "Epoch: 042/500 | Batch 128/275 | Cost: 0.1966\n",
      "Epoch: 042/500 | Batch 192/275 | Cost: 0.2347\n",
      "Epoch: 042/500 | Batch 256/275 | Cost: 0.2940\n",
      "Epoch: 042/500 Train Cost: 0.2586\n",
      "Time elapsed: 1.13 min\n",
      "Epoch: 043/500 | Batch 000/275 | Cost: 0.3458\n",
      "Epoch: 043/500 | Batch 064/275 | Cost: 0.1588\n",
      "Epoch: 043/500 | Batch 128/275 | Cost: 0.1823\n",
      "Epoch: 043/500 | Batch 192/275 | Cost: 0.1514\n",
      "Epoch: 043/500 | Batch 256/275 | Cost: 0.3826\n",
      "Epoch: 043/500 Train Cost: 0.2560\n",
      "Time elapsed: 1.14 min\n",
      "Epoch: 044/500 | Batch 000/275 | Cost: 0.2020\n",
      "Epoch: 044/500 | Batch 064/275 | Cost: 0.3487\n",
      "Epoch: 044/500 | Batch 128/275 | Cost: 0.1627\n",
      "Epoch: 044/500 | Batch 192/275 | Cost: 0.1886\n",
      "Epoch: 044/500 | Batch 256/275 | Cost: 0.1650\n",
      "Epoch: 044/500 Train Cost: 0.2555\n",
      "Time elapsed: 1.16 min\n",
      "Epoch: 045/500 | Batch 000/275 | Cost: 0.3107\n",
      "Epoch: 045/500 | Batch 064/275 | Cost: 0.2121\n",
      "Epoch: 045/500 | Batch 128/275 | Cost: 0.2471\n",
      "Epoch: 045/500 | Batch 192/275 | Cost: 0.2151\n",
      "Epoch: 045/500 | Batch 256/275 | Cost: 0.3169\n",
      "Epoch: 045/500 Train Cost: 0.2586\n",
      "Time elapsed: 1.18 min\n",
      "Epoch: 046/500 | Batch 000/275 | Cost: 0.2405\n",
      "Epoch: 046/500 | Batch 064/275 | Cost: 0.2589\n",
      "Epoch: 046/500 | Batch 128/275 | Cost: 0.1886\n",
      "Epoch: 046/500 | Batch 192/275 | Cost: 0.2519\n",
      "Epoch: 046/500 | Batch 256/275 | Cost: 0.3022\n",
      "Epoch: 046/500 Train Cost: 0.2566\n",
      "Time elapsed: 1.20 min\n",
      "Epoch: 047/500 | Batch 000/275 | Cost: 0.2142\n",
      "Epoch: 047/500 | Batch 064/275 | Cost: 0.2006\n",
      "Epoch: 047/500 | Batch 128/275 | Cost: 0.4908\n",
      "Epoch: 047/500 | Batch 192/275 | Cost: 0.1930\n",
      "Epoch: 047/500 | Batch 256/275 | Cost: 0.4293\n",
      "Epoch: 047/500 Train Cost: 0.2565\n",
      "Time elapsed: 1.22 min\n",
      "Epoch: 048/500 | Batch 000/275 | Cost: 0.1469\n",
      "Epoch: 048/500 | Batch 064/275 | Cost: 0.2949\n",
      "Epoch: 048/500 | Batch 128/275 | Cost: 0.2541\n",
      "Epoch: 048/500 | Batch 192/275 | Cost: 0.3219\n",
      "Epoch: 048/500 | Batch 256/275 | Cost: 0.2452\n",
      "Epoch: 048/500 Train Cost: 0.2534\n",
      "Time elapsed: 1.24 min\n",
      "Epoch: 049/500 | Batch 000/275 | Cost: 0.2551\n",
      "Epoch: 049/500 | Batch 064/275 | Cost: 0.2519\n",
      "Epoch: 049/500 | Batch 128/275 | Cost: 0.2233\n",
      "Epoch: 049/500 | Batch 192/275 | Cost: 0.3588\n",
      "Epoch: 049/500 | Batch 256/275 | Cost: 0.2361\n",
      "Epoch: 049/500 Train Cost: 0.2545\n",
      "Time elapsed: 1.25 min\n",
      "Epoch: 050/500 | Batch 000/275 | Cost: 0.2480\n",
      "Epoch: 050/500 | Batch 064/275 | Cost: 0.2031\n",
      "Epoch: 050/500 | Batch 128/275 | Cost: 0.1828\n",
      "Epoch: 050/500 | Batch 192/275 | Cost: 0.4229\n",
      "Epoch: 050/500 | Batch 256/275 | Cost: 0.3319\n",
      "Epoch: 050/500 Train Cost: 0.2519\n",
      "Time elapsed: 1.27 min\n",
      "Epoch: 051/500 | Batch 000/275 | Cost: 0.2998\n",
      "Epoch: 051/500 | Batch 064/275 | Cost: 0.3064\n",
      "Epoch: 051/500 | Batch 128/275 | Cost: 0.2570\n",
      "Epoch: 051/500 | Batch 192/275 | Cost: 0.4660\n",
      "Epoch: 051/500 | Batch 256/275 | Cost: 0.2046\n",
      "Epoch: 051/500 Train Cost: 0.2576\n",
      "Time elapsed: 1.29 min\n",
      "Epoch: 052/500 | Batch 000/275 | Cost: 0.2675\n",
      "Epoch: 052/500 | Batch 064/275 | Cost: 0.2120\n",
      "Epoch: 052/500 | Batch 128/275 | Cost: 0.1928\n",
      "Epoch: 052/500 | Batch 192/275 | Cost: 0.2404\n",
      "Epoch: 052/500 | Batch 256/275 | Cost: 0.4087\n",
      "Epoch: 052/500 Train Cost: 0.2584\n",
      "Time elapsed: 1.31 min\n",
      "Epoch: 053/500 | Batch 000/275 | Cost: 0.1671\n",
      "Epoch: 053/500 | Batch 064/275 | Cost: 0.2806\n",
      "Epoch: 053/500 | Batch 128/275 | Cost: 0.3049\n",
      "Epoch: 053/500 | Batch 192/275 | Cost: 0.2934\n",
      "Epoch: 053/500 | Batch 256/275 | Cost: 0.4403\n",
      "Epoch: 053/500 Train Cost: 0.2585\n",
      "Time elapsed: 1.33 min\n",
      "Epoch: 054/500 | Batch 000/275 | Cost: 0.1762\n",
      "Epoch: 054/500 | Batch 064/275 | Cost: 0.3523\n",
      "Epoch: 054/500 | Batch 128/275 | Cost: 0.1837\n",
      "Epoch: 054/500 | Batch 192/275 | Cost: 0.2195\n",
      "Epoch: 054/500 | Batch 256/275 | Cost: 0.2903\n",
      "Epoch: 054/500 Train Cost: 0.2583\n",
      "Time elapsed: 1.34 min\n",
      "Epoch: 055/500 | Batch 000/275 | Cost: 0.2147\n",
      "Epoch: 055/500 | Batch 064/275 | Cost: 0.4384\n",
      "Epoch: 055/500 | Batch 128/275 | Cost: 0.2021\n",
      "Epoch: 055/500 | Batch 192/275 | Cost: 0.1870\n",
      "Epoch: 055/500 | Batch 256/275 | Cost: 0.3316\n",
      "Epoch: 055/500 Train Cost: 0.2578\n",
      "Time elapsed: 1.36 min\n",
      "Epoch: 056/500 | Batch 000/275 | Cost: 0.2889\n",
      "Epoch: 056/500 | Batch 064/275 | Cost: 0.2169\n",
      "Epoch: 056/500 | Batch 128/275 | Cost: 0.1792\n",
      "Epoch: 056/500 | Batch 192/275 | Cost: 0.1696\n",
      "Epoch: 056/500 | Batch 256/275 | Cost: 0.2692\n",
      "Epoch: 056/500 Train Cost: 0.2540\n",
      "Time elapsed: 1.38 min\n",
      "Epoch: 057/500 | Batch 000/275 | Cost: 0.2031\n",
      "Epoch: 057/500 | Batch 064/275 | Cost: 0.2405\n",
      "Epoch: 057/500 | Batch 128/275 | Cost: 0.2275\n",
      "Epoch: 057/500 | Batch 192/275 | Cost: 0.3043\n",
      "Epoch: 057/500 | Batch 256/275 | Cost: 0.2099\n",
      "Epoch: 057/500 Train Cost: 0.2510\n",
      "Time elapsed: 1.40 min\n",
      "Epoch: 058/500 | Batch 000/275 | Cost: 0.2860\n",
      "Epoch: 058/500 | Batch 064/275 | Cost: 0.2413\n",
      "Epoch: 058/500 | Batch 128/275 | Cost: 0.3321\n",
      "Epoch: 058/500 | Batch 192/275 | Cost: 0.1800\n",
      "Epoch: 058/500 | Batch 256/275 | Cost: 0.2507\n",
      "Epoch: 058/500 Train Cost: 0.2557\n",
      "Time elapsed: 1.42 min\n",
      "Epoch: 059/500 | Batch 000/275 | Cost: 0.1993\n",
      "Epoch: 059/500 | Batch 064/275 | Cost: 0.1798\n",
      "Epoch: 059/500 | Batch 128/275 | Cost: 0.2403\n",
      "Epoch: 059/500 | Batch 192/275 | Cost: 0.3299\n",
      "Epoch: 059/500 | Batch 256/275 | Cost: 0.1996\n",
      "Epoch: 059/500 Train Cost: 0.2555\n",
      "Time elapsed: 1.44 min\n",
      "Epoch: 060/500 | Batch 000/275 | Cost: 0.1982\n",
      "Epoch: 060/500 | Batch 064/275 | Cost: 0.3985\n",
      "Epoch: 060/500 | Batch 128/275 | Cost: 0.1262\n",
      "Epoch: 060/500 | Batch 192/275 | Cost: 0.3386\n",
      "Epoch: 060/500 | Batch 256/275 | Cost: 0.3070\n",
      "Epoch: 060/500 Train Cost: 0.2532\n",
      "Time elapsed: 1.46 min\n",
      "Epoch: 061/500 | Batch 000/275 | Cost: 0.2028\n",
      "Epoch: 061/500 | Batch 064/275 | Cost: 0.2453\n",
      "Epoch: 061/500 | Batch 128/275 | Cost: 0.2515\n",
      "Epoch: 061/500 | Batch 192/275 | Cost: 0.1433\n",
      "Epoch: 061/500 | Batch 256/275 | Cost: 0.2343\n",
      "Epoch: 061/500 Train Cost: 0.2564\n",
      "Time elapsed: 1.48 min\n",
      "Epoch: 062/500 | Batch 000/275 | Cost: 0.2300\n",
      "Epoch: 062/500 | Batch 064/275 | Cost: 0.2060\n",
      "Epoch: 062/500 | Batch 128/275 | Cost: 0.2318\n",
      "Epoch: 062/500 | Batch 192/275 | Cost: 0.1861\n",
      "Epoch: 062/500 | Batch 256/275 | Cost: 0.2338\n",
      "Epoch: 062/500 Train Cost: 0.2647\n",
      "Time elapsed: 1.50 min\n",
      "Epoch: 063/500 | Batch 000/275 | Cost: 0.2348\n",
      "Epoch: 063/500 | Batch 064/275 | Cost: 0.2083\n",
      "Epoch: 063/500 | Batch 128/275 | Cost: 0.2962\n",
      "Epoch: 063/500 | Batch 192/275 | Cost: 0.1615\n",
      "Epoch: 063/500 | Batch 256/275 | Cost: 0.2067\n",
      "Epoch: 063/500 Train Cost: 0.2531\n",
      "Time elapsed: 1.52 min\n",
      "Epoch: 064/500 | Batch 000/275 | Cost: 0.2760\n",
      "Epoch: 064/500 | Batch 064/275 | Cost: 0.1976\n",
      "Epoch: 064/500 | Batch 128/275 | Cost: 0.1537\n",
      "Epoch: 064/500 | Batch 192/275 | Cost: 0.3124\n",
      "Epoch: 064/500 | Batch 256/275 | Cost: 0.2198\n",
      "Epoch: 064/500 Train Cost: 0.2521\n",
      "Time elapsed: 1.54 min\n",
      "Epoch: 065/500 | Batch 000/275 | Cost: 0.2775\n",
      "Epoch: 065/500 | Batch 064/275 | Cost: 0.5744\n",
      "Epoch: 065/500 | Batch 128/275 | Cost: 0.3878\n",
      "Epoch: 065/500 | Batch 192/275 | Cost: 0.2432\n",
      "Epoch: 065/500 | Batch 256/275 | Cost: 0.3185\n",
      "Epoch: 065/500 Train Cost: 0.2522\n",
      "Time elapsed: 1.56 min\n",
      "Epoch: 066/500 | Batch 000/275 | Cost: 0.3591\n",
      "Epoch: 066/500 | Batch 064/275 | Cost: 0.2582\n",
      "Epoch: 066/500 | Batch 128/275 | Cost: 0.2425\n",
      "Epoch: 066/500 | Batch 192/275 | Cost: 0.2282\n",
      "Epoch: 066/500 | Batch 256/275 | Cost: 0.1460\n",
      "Epoch: 066/500 Train Cost: 0.2505\n",
      "Time elapsed: 1.57 min\n",
      "Epoch: 067/500 | Batch 000/275 | Cost: 0.2536\n",
      "Epoch: 067/500 | Batch 064/275 | Cost: 0.2981\n",
      "Epoch: 067/500 | Batch 128/275 | Cost: 0.3549\n",
      "Epoch: 067/500 | Batch 192/275 | Cost: 0.1753\n",
      "Epoch: 067/500 | Batch 256/275 | Cost: 0.2279\n",
      "Epoch: 067/500 Train Cost: 0.2528\n",
      "Time elapsed: 1.59 min\n",
      "Epoch: 068/500 | Batch 000/275 | Cost: 0.3203\n",
      "Epoch: 068/500 | Batch 064/275 | Cost: 0.2979\n",
      "Epoch: 068/500 | Batch 128/275 | Cost: 0.2755\n",
      "Epoch: 068/500 | Batch 192/275 | Cost: 0.1691\n",
      "Epoch: 068/500 | Batch 256/275 | Cost: 0.3484\n",
      "Epoch: 068/500 Train Cost: 0.2486\n",
      "Time elapsed: 1.61 min\n",
      "Epoch: 069/500 | Batch 000/275 | Cost: 0.2132\n",
      "Epoch: 069/500 | Batch 064/275 | Cost: 0.2309\n",
      "Epoch: 069/500 | Batch 128/275 | Cost: 0.2618\n",
      "Epoch: 069/500 | Batch 192/275 | Cost: 0.2579\n",
      "Epoch: 069/500 | Batch 256/275 | Cost: 0.4681\n",
      "Epoch: 069/500 Train Cost: 0.2508\n",
      "Time elapsed: 1.63 min\n",
      "Epoch: 070/500 | Batch 000/275 | Cost: 0.1717\n",
      "Epoch: 070/500 | Batch 064/275 | Cost: 0.2468\n",
      "Epoch: 070/500 | Batch 128/275 | Cost: 0.2663\n",
      "Epoch: 070/500 | Batch 192/275 | Cost: 0.2155\n",
      "Epoch: 070/500 | Batch 256/275 | Cost: 0.2447\n",
      "Epoch: 070/500 Train Cost: 0.2511\n",
      "Time elapsed: 1.68 min\n",
      "Epoch: 071/500 | Batch 000/275 | Cost: 0.1724\n",
      "Epoch: 071/500 | Batch 064/275 | Cost: 0.3704\n",
      "Epoch: 071/500 | Batch 128/275 | Cost: 0.1289\n",
      "Epoch: 071/500 | Batch 192/275 | Cost: 0.3043\n",
      "Epoch: 071/500 | Batch 256/275 | Cost: 0.2572\n",
      "Epoch: 071/500 Train Cost: 0.2523\n",
      "Time elapsed: 1.76 min\n",
      "Epoch: 072/500 | Batch 000/275 | Cost: 0.2726\n",
      "Epoch: 072/500 | Batch 064/275 | Cost: 0.2794\n",
      "Epoch: 072/500 | Batch 128/275 | Cost: 0.1997\n",
      "Epoch: 072/500 | Batch 192/275 | Cost: 0.2151\n",
      "Epoch: 072/500 | Batch 256/275 | Cost: 0.1899\n",
      "Epoch: 072/500 Train Cost: 0.2522\n",
      "Time elapsed: 1.78 min\n",
      "Epoch: 073/500 | Batch 000/275 | Cost: 0.4207\n",
      "Epoch: 073/500 | Batch 064/275 | Cost: 0.1087\n",
      "Epoch: 073/500 | Batch 128/275 | Cost: 0.3144\n",
      "Epoch: 073/500 | Batch 192/275 | Cost: 0.2619\n",
      "Epoch: 073/500 | Batch 256/275 | Cost: 0.2251\n",
      "Epoch: 073/500 Train Cost: 0.2525\n",
      "Time elapsed: 1.80 min\n",
      "Epoch: 074/500 | Batch 000/275 | Cost: 0.2321\n",
      "Epoch: 074/500 | Batch 064/275 | Cost: 0.2685\n",
      "Epoch: 074/500 | Batch 128/275 | Cost: 0.2074\n",
      "Epoch: 074/500 | Batch 192/275 | Cost: 0.1700\n",
      "Epoch: 074/500 | Batch 256/275 | Cost: 0.3420\n",
      "Epoch: 074/500 Train Cost: 0.2515\n",
      "Time elapsed: 1.82 min\n",
      "Epoch: 075/500 | Batch 000/275 | Cost: 0.3251\n",
      "Epoch: 075/500 | Batch 064/275 | Cost: 0.3215\n",
      "Epoch: 075/500 | Batch 128/275 | Cost: 0.2976\n",
      "Epoch: 075/500 | Batch 192/275 | Cost: 0.1501\n",
      "Epoch: 075/500 | Batch 256/275 | Cost: 0.5218\n",
      "Epoch: 075/500 Train Cost: 0.2486\n",
      "Time elapsed: 1.84 min\n",
      "Epoch: 076/500 | Batch 000/275 | Cost: 0.2503\n",
      "Epoch: 076/500 | Batch 064/275 | Cost: 0.2925\n",
      "Epoch: 076/500 | Batch 128/275 | Cost: 0.1805\n",
      "Epoch: 076/500 | Batch 192/275 | Cost: 0.1735\n",
      "Epoch: 076/500 | Batch 256/275 | Cost: 0.4699\n",
      "Epoch: 076/500 Train Cost: 0.2532\n",
      "Time elapsed: 1.86 min\n",
      "Epoch: 077/500 | Batch 000/275 | Cost: 0.2575\n",
      "Epoch: 077/500 | Batch 064/275 | Cost: 0.6850\n",
      "Epoch: 077/500 | Batch 128/275 | Cost: 0.3327\n",
      "Epoch: 077/500 | Batch 192/275 | Cost: 0.2642\n",
      "Epoch: 077/500 | Batch 256/275 | Cost: 0.4440\n",
      "Epoch: 077/500 Train Cost: 0.2458\n",
      "Time elapsed: 1.87 min\n",
      "Epoch: 078/500 | Batch 000/275 | Cost: 0.1657\n",
      "Epoch: 078/500 | Batch 064/275 | Cost: 0.1704\n",
      "Epoch: 078/500 | Batch 128/275 | Cost: 0.2475\n",
      "Epoch: 078/500 | Batch 192/275 | Cost: 0.1759\n",
      "Epoch: 078/500 | Batch 256/275 | Cost: 0.2762\n",
      "Epoch: 078/500 Train Cost: 0.2507\n",
      "Time elapsed: 1.89 min\n",
      "Epoch: 079/500 | Batch 000/275 | Cost: 0.3287\n",
      "Epoch: 079/500 | Batch 064/275 | Cost: 0.2642\n",
      "Epoch: 079/500 | Batch 128/275 | Cost: 0.3084\n",
      "Epoch: 079/500 | Batch 192/275 | Cost: 0.3484\n",
      "Epoch: 079/500 | Batch 256/275 | Cost: 0.1772\n",
      "Epoch: 079/500 Train Cost: 0.2499\n",
      "Time elapsed: 1.91 min\n",
      "Epoch: 080/500 | Batch 000/275 | Cost: 0.4642\n",
      "Epoch: 080/500 | Batch 064/275 | Cost: 0.3506\n",
      "Epoch: 080/500 | Batch 128/275 | Cost: 0.3172\n",
      "Epoch: 080/500 | Batch 192/275 | Cost: 0.1761\n",
      "Epoch: 080/500 | Batch 256/275 | Cost: 0.2593\n",
      "Epoch: 080/500 Train Cost: 0.2489\n",
      "Time elapsed: 1.93 min\n",
      "Epoch: 081/500 | Batch 000/275 | Cost: 0.1736\n",
      "Epoch: 081/500 | Batch 064/275 | Cost: 0.1344\n",
      "Epoch: 081/500 | Batch 128/275 | Cost: 0.2892\n",
      "Epoch: 081/500 | Batch 192/275 | Cost: 0.2617\n",
      "Epoch: 081/500 | Batch 256/275 | Cost: 0.1382\n",
      "Epoch: 081/500 Train Cost: 0.2466\n",
      "Time elapsed: 1.95 min\n",
      "Epoch: 082/500 | Batch 000/275 | Cost: 0.2837\n",
      "Epoch: 082/500 | Batch 064/275 | Cost: 0.2451\n",
      "Epoch: 082/500 | Batch 128/275 | Cost: 0.3881\n",
      "Epoch: 082/500 | Batch 192/275 | Cost: 0.1720\n",
      "Epoch: 082/500 | Batch 256/275 | Cost: 0.2385\n",
      "Epoch: 082/500 Train Cost: 0.2496\n",
      "Time elapsed: 2.02 min\n",
      "Epoch: 083/500 | Batch 000/275 | Cost: 0.3113\n",
      "Epoch: 083/500 | Batch 064/275 | Cost: 0.2626\n",
      "Epoch: 083/500 | Batch 128/275 | Cost: 0.2772\n",
      "Epoch: 083/500 | Batch 192/275 | Cost: 0.1678\n",
      "Epoch: 083/500 | Batch 256/275 | Cost: 0.2192\n",
      "Epoch: 083/500 Train Cost: 0.2468\n",
      "Time elapsed: 2.12 min\n",
      "Epoch: 084/500 | Batch 000/275 | Cost: 0.1745\n",
      "Epoch: 084/500 | Batch 064/275 | Cost: 0.4603\n",
      "Epoch: 084/500 | Batch 128/275 | Cost: 0.2487\n",
      "Epoch: 084/500 | Batch 192/275 | Cost: 0.1789\n",
      "Epoch: 084/500 | Batch 256/275 | Cost: 0.2038\n",
      "Epoch: 084/500 Train Cost: 0.2498\n",
      "Time elapsed: 2.22 min\n",
      "Epoch: 085/500 | Batch 000/275 | Cost: 0.2735\n",
      "Epoch: 085/500 | Batch 064/275 | Cost: 0.1653\n",
      "Epoch: 085/500 | Batch 128/275 | Cost: 0.1845\n",
      "Epoch: 085/500 | Batch 192/275 | Cost: 0.1907\n",
      "Epoch: 085/500 | Batch 256/275 | Cost: 0.2477\n",
      "Epoch: 085/500 Train Cost: 0.2472\n",
      "Time elapsed: 2.31 min\n",
      "Epoch: 086/500 | Batch 000/275 | Cost: 0.2227\n",
      "Epoch: 086/500 | Batch 064/275 | Cost: 0.1625\n",
      "Epoch: 086/500 | Batch 128/275 | Cost: 0.1753\n",
      "Epoch: 086/500 | Batch 192/275 | Cost: 0.3042\n",
      "Epoch: 086/500 | Batch 256/275 | Cost: 0.4333\n",
      "Epoch: 086/500 Train Cost: 0.2479\n",
      "Time elapsed: 2.40 min\n",
      "Epoch: 087/500 | Batch 000/275 | Cost: 0.2229\n",
      "Epoch: 087/500 | Batch 064/275 | Cost: 0.2649\n",
      "Epoch: 087/500 | Batch 128/275 | Cost: 0.2548\n",
      "Epoch: 087/500 | Batch 192/275 | Cost: 0.1761\n",
      "Epoch: 087/500 | Batch 256/275 | Cost: 0.1571\n",
      "Epoch: 087/500 Train Cost: 0.2453\n",
      "Time elapsed: 2.49 min\n",
      "Epoch: 088/500 | Batch 000/275 | Cost: 0.1791\n",
      "Epoch: 088/500 | Batch 064/275 | Cost: 0.3715\n",
      "Epoch: 088/500 | Batch 128/275 | Cost: 0.2169\n",
      "Epoch: 088/500 | Batch 192/275 | Cost: 0.2381\n",
      "Epoch: 088/500 | Batch 256/275 | Cost: 0.1736\n",
      "Epoch: 088/500 Train Cost: 0.2515\n",
      "Time elapsed: 2.58 min\n",
      "Epoch: 089/500 | Batch 000/275 | Cost: 0.1847\n",
      "Epoch: 089/500 | Batch 064/275 | Cost: 0.1721\n",
      "Epoch: 089/500 | Batch 128/275 | Cost: 0.2201\n",
      "Epoch: 089/500 | Batch 192/275 | Cost: 0.2912\n",
      "Epoch: 089/500 | Batch 256/275 | Cost: 0.3088\n",
      "Epoch: 089/500 Train Cost: 0.2473\n",
      "Time elapsed: 2.67 min\n",
      "Epoch: 090/500 | Batch 000/275 | Cost: 0.3066\n",
      "Epoch: 090/500 | Batch 064/275 | Cost: 0.5753\n",
      "Epoch: 090/500 | Batch 128/275 | Cost: 0.1831\n",
      "Epoch: 090/500 | Batch 192/275 | Cost: 0.3258\n",
      "Epoch: 090/500 | Batch 256/275 | Cost: 0.2545\n",
      "Epoch: 090/500 Train Cost: 0.2468\n",
      "Time elapsed: 2.75 min\n",
      "Epoch: 091/500 | Batch 000/275 | Cost: 0.1521\n",
      "Epoch: 091/500 | Batch 064/275 | Cost: 0.1972\n",
      "Epoch: 091/500 | Batch 128/275 | Cost: 0.3479\n",
      "Epoch: 091/500 | Batch 192/275 | Cost: 0.3415\n",
      "Epoch: 091/500 | Batch 256/275 | Cost: 0.1386\n",
      "Epoch: 091/500 Train Cost: 0.2477\n",
      "Time elapsed: 2.83 min\n",
      "Epoch: 092/500 | Batch 000/275 | Cost: 0.2322\n",
      "Epoch: 092/500 | Batch 064/275 | Cost: 0.3149\n",
      "Epoch: 092/500 | Batch 128/275 | Cost: 0.3341\n",
      "Epoch: 092/500 | Batch 192/275 | Cost: 0.1879\n",
      "Epoch: 092/500 | Batch 256/275 | Cost: 0.1712\n",
      "Epoch: 092/500 Train Cost: 0.2479\n",
      "Time elapsed: 2.91 min\n",
      "Epoch: 093/500 | Batch 000/275 | Cost: 0.1610\n",
      "Epoch: 093/500 | Batch 064/275 | Cost: 0.3048\n",
      "Epoch: 093/500 | Batch 128/275 | Cost: 0.2790\n",
      "Epoch: 093/500 | Batch 192/275 | Cost: 0.2060\n",
      "Epoch: 093/500 | Batch 256/275 | Cost: 0.1937\n",
      "Epoch: 093/500 Train Cost: 0.2468\n",
      "Time elapsed: 3.01 min\n",
      "Epoch: 094/500 | Batch 000/275 | Cost: 0.2673\n",
      "Epoch: 094/500 | Batch 064/275 | Cost: 0.3103\n",
      "Epoch: 094/500 | Batch 128/275 | Cost: 0.1998\n",
      "Epoch: 094/500 | Batch 192/275 | Cost: 0.2607\n",
      "Epoch: 094/500 | Batch 256/275 | Cost: 0.1963\n",
      "Epoch: 094/500 Train Cost: 0.2462\n",
      "Time elapsed: 3.10 min\n",
      "Epoch: 095/500 | Batch 000/275 | Cost: 0.1779\n",
      "Epoch: 095/500 | Batch 064/275 | Cost: 0.2326\n",
      "Epoch: 095/500 | Batch 128/275 | Cost: 0.1745\n",
      "Epoch: 095/500 | Batch 192/275 | Cost: 0.2646\n",
      "Epoch: 095/500 | Batch 256/275 | Cost: 0.2089\n",
      "Epoch: 095/500 Train Cost: 0.2511\n",
      "Time elapsed: 3.18 min\n",
      "Epoch: 096/500 | Batch 000/275 | Cost: 0.5679\n",
      "Epoch: 096/500 | Batch 064/275 | Cost: 0.2827\n",
      "Epoch: 096/500 | Batch 128/275 | Cost: 0.1669\n",
      "Epoch: 096/500 | Batch 192/275 | Cost: 0.1759\n",
      "Epoch: 096/500 | Batch 256/275 | Cost: 0.2370\n",
      "Epoch: 096/500 Train Cost: 0.2453\n",
      "Time elapsed: 3.26 min\n",
      "Epoch: 097/500 | Batch 000/275 | Cost: 0.2000\n",
      "Epoch: 097/500 | Batch 064/275 | Cost: 0.2238\n",
      "Epoch: 097/500 | Batch 128/275 | Cost: 0.2925\n",
      "Epoch: 097/500 | Batch 192/275 | Cost: 0.1883\n",
      "Epoch: 097/500 | Batch 256/275 | Cost: 0.1707\n",
      "Epoch: 097/500 Train Cost: 0.2531\n",
      "Time elapsed: 3.33 min\n",
      "Epoch: 098/500 | Batch 000/275 | Cost: 0.2721\n",
      "Epoch: 098/500 | Batch 064/275 | Cost: 0.2395\n",
      "Epoch: 098/500 | Batch 128/275 | Cost: 0.4794\n",
      "Epoch: 098/500 | Batch 192/275 | Cost: 0.1796\n",
      "Epoch: 098/500 | Batch 256/275 | Cost: 0.3482\n",
      "Epoch: 098/500 Train Cost: 0.2485\n",
      "Time elapsed: 3.34 min\n",
      "Epoch: 099/500 | Batch 000/275 | Cost: 0.2432\n",
      "Epoch: 099/500 | Batch 064/275 | Cost: 0.3416\n",
      "Epoch: 099/500 | Batch 128/275 | Cost: 0.3490\n",
      "Epoch: 099/500 | Batch 192/275 | Cost: 0.4541\n",
      "Epoch: 099/500 | Batch 256/275 | Cost: 0.2647\n",
      "Epoch: 099/500 Train Cost: 0.2446\n",
      "Time elapsed: 3.36 min\n",
      "Epoch: 100/500 | Batch 000/275 | Cost: 0.3829\n",
      "Epoch: 100/500 | Batch 064/275 | Cost: 0.1937\n",
      "Epoch: 100/500 | Batch 128/275 | Cost: 0.1926\n",
      "Epoch: 100/500 | Batch 192/275 | Cost: 0.1598\n",
      "Epoch: 100/500 | Batch 256/275 | Cost: 0.2176\n",
      "Epoch: 100/500 Train Cost: 0.2476\n",
      "Time elapsed: 3.38 min\n",
      "Epoch: 101/500 | Batch 000/275 | Cost: 0.2075\n",
      "Epoch: 101/500 | Batch 064/275 | Cost: 0.4199\n",
      "Epoch: 101/500 | Batch 128/275 | Cost: 0.1162\n",
      "Epoch: 101/500 | Batch 192/275 | Cost: 0.4055\n",
      "Epoch: 101/500 | Batch 256/275 | Cost: 0.4520\n",
      "Epoch: 101/500 Train Cost: 0.2542\n",
      "Time elapsed: 3.40 min\n",
      "Epoch: 102/500 | Batch 000/275 | Cost: 0.1793\n",
      "Epoch: 102/500 | Batch 064/275 | Cost: 0.2736\n",
      "Epoch: 102/500 | Batch 128/275 | Cost: 0.4007\n",
      "Epoch: 102/500 | Batch 192/275 | Cost: 0.2543\n",
      "Epoch: 102/500 | Batch 256/275 | Cost: 0.2741\n",
      "Epoch: 102/500 Train Cost: 0.2474\n",
      "Time elapsed: 3.42 min\n",
      "Epoch: 103/500 | Batch 000/275 | Cost: 0.3321\n",
      "Epoch: 103/500 | Batch 064/275 | Cost: 0.2115\n",
      "Epoch: 103/500 | Batch 128/275 | Cost: 0.2650\n",
      "Epoch: 103/500 | Batch 192/275 | Cost: 0.2863\n",
      "Epoch: 103/500 | Batch 256/275 | Cost: 0.2597\n",
      "Epoch: 103/500 Train Cost: 0.2493\n",
      "Time elapsed: 3.44 min\n",
      "Epoch: 104/500 | Batch 000/275 | Cost: 0.3221\n",
      "Epoch: 104/500 | Batch 064/275 | Cost: 0.2514\n",
      "Epoch: 104/500 | Batch 128/275 | Cost: 0.2834\n",
      "Epoch: 104/500 | Batch 192/275 | Cost: 0.1140\n",
      "Epoch: 104/500 | Batch 256/275 | Cost: 0.3574\n",
      "Epoch: 104/500 Train Cost: 0.2492\n",
      "Time elapsed: 3.46 min\n",
      "Epoch: 105/500 | Batch 000/275 | Cost: 0.3134\n",
      "Epoch: 105/500 | Batch 064/275 | Cost: 0.2852\n",
      "Epoch: 105/500 | Batch 128/275 | Cost: 0.3346\n",
      "Epoch: 105/500 | Batch 192/275 | Cost: 0.3599\n",
      "Epoch: 105/500 | Batch 256/275 | Cost: 0.1578\n",
      "Epoch: 105/500 Train Cost: 0.2477\n",
      "Time elapsed: 3.48 min\n",
      "Epoch: 106/500 | Batch 000/275 | Cost: 0.3300\n",
      "Epoch: 106/500 | Batch 064/275 | Cost: 0.1675\n",
      "Epoch: 106/500 | Batch 128/275 | Cost: 0.2282\n",
      "Epoch: 106/500 | Batch 192/275 | Cost: 0.1814\n",
      "Epoch: 106/500 | Batch 256/275 | Cost: 0.1650\n",
      "Epoch: 106/500 Train Cost: 0.2480\n",
      "Time elapsed: 3.50 min\n",
      "Epoch: 107/500 | Batch 000/275 | Cost: 0.2929\n",
      "Epoch: 107/500 | Batch 064/275 | Cost: 0.4440\n",
      "Epoch: 107/500 | Batch 128/275 | Cost: 0.1247\n",
      "Epoch: 107/500 | Batch 192/275 | Cost: 0.1955\n",
      "Epoch: 107/500 | Batch 256/275 | Cost: 0.1574\n",
      "Epoch: 107/500 Train Cost: 0.2467\n",
      "Time elapsed: 3.52 min\n",
      "Epoch: 108/500 | Batch 000/275 | Cost: 0.1505\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[22]\u001B[39m\u001B[32m, line 52\u001B[39m\n\u001B[32m     50\u001B[39m optimizer.zero_grad()\n\u001B[32m     51\u001B[39m cost.backward()\n\u001B[32m---> \u001B[39m\u001B[32m52\u001B[39m \u001B[43moptimizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     53\u001B[39m minibatch_cost.append(cost.item())\n\u001B[32m     54\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m batch_idx % \u001B[32m64\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:526\u001B[39m, in \u001B[36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    521\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m    522\u001B[39m                 \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    523\u001B[39m             )\n\u001B[32m    525\u001B[39m \u001B[38;5;66;03m# pyrefly: ignore [invalid-param-spec]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m526\u001B[39m out = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    527\u001B[39m \u001B[38;5;28mself\u001B[39m._optimizer_step_code()\n\u001B[32m    529\u001B[39m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:81\u001B[39m, in \u001B[36m_use_grad_for_differentiable.<locals>._use_grad\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m     79\u001B[39m     torch.set_grad_enabled(\u001B[38;5;28mself\u001B[39m.defaults[\u001B[33m\"\u001B[39m\u001B[33mdifferentiable\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m     80\u001B[39m     torch._dynamo.graph_break()\n\u001B[32m---> \u001B[39m\u001B[32m81\u001B[39m     ret = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     82\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m     83\u001B[39m     torch._dynamo.graph_break()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:248\u001B[39m, in \u001B[36mAdam.step\u001B[39m\u001B[34m(self, closure)\u001B[39m\n\u001B[32m    236\u001B[39m     beta1, beta2 = group[\u001B[33m\"\u001B[39m\u001B[33mbetas\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m    238\u001B[39m     has_complex = \u001B[38;5;28mself\u001B[39m._init_group(\n\u001B[32m    239\u001B[39m         group,\n\u001B[32m    240\u001B[39m         params_with_grad,\n\u001B[32m   (...)\u001B[39m\u001B[32m    245\u001B[39m         state_steps,\n\u001B[32m    246\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m248\u001B[39m     \u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    249\u001B[39m \u001B[43m        \u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    250\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    251\u001B[39m \u001B[43m        \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    252\u001B[39m \u001B[43m        \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    253\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    254\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    255\u001B[39m \u001B[43m        \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mamsgrad\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    256\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    257\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    258\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    259\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mlr\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    260\u001B[39m \u001B[43m        \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mweight_decay\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    261\u001B[39m \u001B[43m        \u001B[49m\u001B[43meps\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43meps\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    262\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmaximize\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    263\u001B[39m \u001B[43m        \u001B[49m\u001B[43mforeach\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mforeach\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    264\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcapturable\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    265\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mdifferentiable\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    266\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfused\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfused\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    267\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mgrad_scale\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    268\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfound_inf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    269\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdecoupled_weight_decay\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mdecoupled_weight_decay\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    270\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    272\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:151\u001B[39m, in \u001B[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    149\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m disabled_func(*args, **kwargs)\n\u001B[32m    150\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m151\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:970\u001B[39m, in \u001B[36madam\u001B[39m\u001B[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[39m\n\u001B[32m    967\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    968\u001B[39m     func = _single_tensor_adam\n\u001B[32m--> \u001B[39m\u001B[32m970\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    971\u001B[39m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    972\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    973\u001B[39m \u001B[43m    \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    974\u001B[39m \u001B[43m    \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    975\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    976\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    977\u001B[39m \u001B[43m    \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[43m=\u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    978\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    979\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    980\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    981\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    982\u001B[39m \u001B[43m    \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[43m=\u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    983\u001B[39m \u001B[43m    \u001B[49m\u001B[43meps\u001B[49m\u001B[43m=\u001B[49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    984\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    985\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    986\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    987\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    988\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    989\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdecoupled_weight_decay\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecoupled_weight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    990\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:545\u001B[39m, in \u001B[36m_single_tensor_adam\u001B[39m\u001B[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001B[39m\n\u001B[32m    543\u001B[39m         denom = (max_exp_avg_sqs[i].sqrt() / bias_correction2_sqrt).add_(eps)\n\u001B[32m    544\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m545\u001B[39m         denom = \u001B[43m(\u001B[49m\u001B[43mexp_avg_sq\u001B[49m\u001B[43m.\u001B[49m\u001B[43msqrt\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m/\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias_correction2_sqrt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43madd_\u001B[49m\u001B[43m(\u001B[49m\u001B[43meps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    547\u001B[39m     param.addcdiv_(exp_avg, denom, value=-step_size)  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[32m    549\u001B[39m \u001B[38;5;66;03m# Lastly, switch back to complex view\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b9968887d8be3800"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
